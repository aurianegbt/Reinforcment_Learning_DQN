{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u> 3. DQN algorithme avec récupération d'expérience </u> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant implémenter l'algorithme de Deep $Q$-learning, celui-ci suit le principe suivant : \n",
    "\n",
    "> <u> <b> Initialisation : </b> </u> On initialise la mémoire $M$ sur les expériences passées sur une liste vide, et on fixe la taille maximale de cette mémoire. \n",
    "\n",
    "> <u> <b> A chaque étape tu temps $t\\in\\mathbb N$, on repète :  </b> </u>\n",
    ">      \n",
    "> * On détermine la $Q$-Valeur de l'état courant $S_t$ :\n",
    "$$ Q^{\\pi} : (S_t,A_t) \\mapsto \\mathbb{E}_{\\pi} \\displaystyle\\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t, A_t  \\right]$$\n",
    ">\n",
    "> * On choisi $A_t$ maximisant $a\\mapsto Q(S_t,a)$.\n",
    ">\n",
    "> * On calcule la récompense associée $R_t=R(S_t,S_t)$\n",
    ">\n",
    "> * On ajoute l'expérience $(S_t, A_t, R_t, S_{t+1})$ à la mémoire $M$ \n",
    ">\n",
    "> * On extrait un batch aléatoire $B\\subset M$ de notre mémoire. Puis pour tout $(S_t^B,A_t^B,R_t^B,S_{t+1}^B)$ dans B :\n",
    "> \n",
    ">      - On calcule $Q(S_t^B,A_t^B)$ et $R(S_t^B,A_t^B)+\\gamma \\max_a Q(S_{t+1}^B,a)$\n",
    ">\n",
    "> * On calcule la perte obtenu sur tout le batch : $$ \\frac 1 2 \\sum_B \\displaystyle \\left(R(S_t^B,A_t^B)+\\gamma \\max_a Q(S_{t+1}^B,a) - Q(S_t^B,A_t^B)\\right)^2$$\n",
    "> \n",
    "> * On utilise cette perte pour mettre à jour les poids du réseau de neuronnes. \n",
    "\n",
    "\n",
    "L'utilisation de mini-batchs permet d'améliorer le processus d'apprentissage en réutilisant des sous-ensembles d'expérience passée pour mettre à jour les poids du réseau de neuronne plutôt que de tout réutiliser. La première raison est le gain de temps de calcul tout en gardant un processus d'apprentissage fiable, permettant en bon compromis entre robustesse et rapidité. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, pour l'implémentation, on procède de la même manière que pour les script précédents : on définit une classe `DQN` qui contiendra notre algorithme d'apprentissage avec expérience. On la définit comme suit : \n",
    "\n",
    "<b> a. </b>  On initialise par `__init__` les 3 éléments indispensables de l'algorithme : la mémoire, la capacité de celle-ci et le facteur $\\gamma$. On initialise ces deux derniers par défault à $100$ et $0.9$ d'après la littérature. \n",
    "\n",
    "<b> b. </b> On construit ensuite une fonction permettant de mettre en mémoire une nouvelle expérience. Celle-ci doit se rappeler de l'expérience $(S_t,A_t,R_t,S_{t+1})$ mais aussi de l'issu de celle-ci, à savoir si `game_over=True` ou `game_over=False`.\n",
    "\n",
    "<b> c. </b> On souhaite, d'après ce qui a été énoncé précédemment, extraire des batchs d'expériences. On implémente ceci dans la fonction `get_batch`. Celle-ci prends en argument notre modèle de réseau de neuronne défini dans $\\texttt{cnn}.\\texttt{ipynb}$, et la taille des batchs souhaitée. On initialise ce batch par un tableau numpy de la taille souhaitée (ou toute la mémoire si la taille est plus grande).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self,memory_capacity=100,gamma=0.9):\n",
    "        self.memory = []\n",
    "        self.memory_capacity = memory_capacity\n",
    "        self.discount = gamma\n",
    "\n",
    "    def save_exp(self,exp,game_over):   #La variable exp=[St,At,Rt,St+1]\n",
    "        self.memory.append([exp,game_over])\n",
    "        if len(self.memory) > self.memory_capacity :\n",
    "            self.memory.pop(0)\n",
    "\n",
    "    def get_batch(self,model, size_batch):\n",
    "        # dans la mémoire : M[i]=[[St, At, Rt, St+1], game_over]\n",
    "\n",
    "        n_memory = len(self.memory)\n",
    "        n_actions = model.output_shape[-1] #méthode de Seuqential() \n",
    "\n",
    "        size_limit = min(n_memory,size_batch)\n",
    "\n",
    "        states = np.zeros((size_limit,self.memory[0][0][0].shape[1],self.memory[0][0][0].shape[2],self.memory[0][0][0].shape[3]))  # on créer un tableau 4D pour extraire size_limit vecteur 3D car les éléments dans memory sont de la forme [[St,At,Rt,St+1],done] Donc memory[0][0][0]=St et on décide que :\n",
    "        # St est un vecteur 3D : une dimension pour les lignes de la grilles, une pour les colonnes des grilles\n",
    "        # St retient l'état de la grille des n dernières actions (d'où la troisième dimension de St \n",
    "        targets = np.zeros((size_limit,n_actions))\n",
    "\n",
    "        batch_set=np.random.randint(0,n_memory,size=size_limit)\n",
    "        for i,ind in enumerate(batch_set):\n",
    "            St, At, Rt, Stp1 = self.memory[ind][0]\n",
    "            game_over = self.memory[ind][1]\n",
    "            states[i] = St\n",
    "            targets[i] = model.predict(St)[0]\n",
    "            Q = np.max(model.predict(Stp1)[0])\n",
    "\n",
    "            if game_over:\n",
    "                targets[i,At] = Rt\n",
    "            else :\n",
    "                targets[i,At] = Rt + self.discount * Q\n",
    "\n",
    "        return(states,targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
